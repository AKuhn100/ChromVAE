{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mdtraj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmdtraj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mdtraj'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import mdtraj as md\n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list, fcluster,leaders\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import random\n",
    "import palettable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set multiprocessing start method for macOS compatibility\n",
    "# Try to use 'fork' if available, otherwise use 'spawn'\n",
    "try:\n",
    "    multiprocessing.set_start_method('fork', force=True)\n",
    "except RuntimeError:\n",
    "    # If fork is not available, use spawn (default on macOS)\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass  # Already set, continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_Q(XYZ_r,XYZ_t,length,sigma=1.0):\n",
    "#     return (np.divide(np.sum(np.exp(-(np.power(distance.cdist(XYZ_r,XYZ_r, 'euclidean')-distance.cdist(XYZ_t,XYZ_t, 'euclidean') ,2))/(2.0*np.power(sigma,2)))),length*(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q - Wolynes\n",
    "\n",
    "def compute_Q(XYZ_r, XYZ_t, length, N, M, sigma=1.0):\n",
    "    d1 = distance.cdist(XYZ_r, XYZ_r, 'euclidean')\n",
    "    d2 = distance.cdist(XYZ_t, XYZ_t, 'euclidean')\n",
    "\n",
    "    # Set the diagonal elements up to N diags with np.inf (close neighbors)\n",
    "    for i in range(N):\n",
    "        if i < length:\n",
    "            np.fill_diagonal(d1[:, i:], np.inf)\n",
    "            np.fill_diagonal(d1[i:, :], np.inf)\n",
    "            np.fill_diagonal(d2[:, i:], np.inf)\n",
    "            np.fill_diagonal(d2[i:, :], np.inf)\n",
    "    \n",
    "    # Set the diagonal elements beyond M diags with np.inf (long-range neighbors)\n",
    "    for i in range(M, length):\n",
    "        np.fill_diagonal(d1[:, i:], np.inf)\n",
    "        np.fill_diagonal(d1[i:, :], np.inf)\n",
    "        np.fill_diagonal(d2[:, i:], np.inf)\n",
    "        np.fill_diagonal(d2[i:, :], np.inf)\n",
    "\n",
    "    # Replace nan values with 0\n",
    "    d1 = np.nan_to_num(d1, posinf=0)\n",
    "    d2 = np.nan_to_num(d2, posinf=0)\n",
    "\n",
    "    return np.divide(np.sum(np.exp(-(np.power(d1 - d2, 2)) / (2.0 * np.power(sigma, 2)))), length * length)\n",
    "\n",
    "\n",
    "# def compute_Q(XYZ_r, XYZ_t, length, N, M, sigma=1.0):\n",
    "#     # Compute pairwise euclidean distances\n",
    "#     d1 = distance.cdist(XYZ_r, XYZ_r, 'euclidean')\n",
    "#     d2 = distance.cdist(XYZ_t, XYZ_t, 'euclidean')\n",
    "\n",
    "#     # Mask close neighbors and long-range neighbors\n",
    "#     for i in range(length):\n",
    "#         if i < N or i >= M:\n",
    "#             d1[i:, i] = np.inf\n",
    "#             d1[i, i:] = np.inf\n",
    "#             d2[i:, i] = np.inf\n",
    "#             d2[i, i:] = np.inf\n",
    "\n",
    "#     # Replace np.inf with 0 for subsequent calculations\n",
    "#     d1 = np.nan_to_num(d1, posinf=0)\n",
    "#     d2 = np.nan_to_num(d2, posinf=0)\n",
    "\n",
    "#     # Calculate the Q metric\n",
    "#     diff_squared = np.power(d1 - d2, 2)\n",
    "#     return np.sum(np.exp(-diff_squared / (2 * sigma ** 2))) / (length ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q - Onuchic\n",
    "# def compute_Q(XYZ_r, XYZ_t, length, N, M,  sigma=2.0):\n",
    "\n",
    "#     d1 = distance.cdist(XYZ_r, XYZ_r, 'euclidean')\n",
    "#     d2 = distance.cdist(XYZ_t, XYZ_t, 'euclidean')\n",
    "    \n",
    "#     # Ignore close neighbors up to N diags by setting them to a large value\n",
    "#     for i in range(N):\n",
    "#         if i < length:\n",
    "#             np.fill_diagonal(d1[:, i:], float('inf'))\n",
    "#             np.fill_diagonal(d1[i:, :], float('inf'))\n",
    "#             np.fill_diagonal(d2[:, i:], float('inf'))\n",
    "#             np.fill_diagonal(d2[i:, :], float('inf'))\n",
    "    \n",
    "#     # Ignore long-range neighbors beyond M diags by setting them to a large value\n",
    "#     for i in range(M, length):\n",
    "#         np.fill_diagonal(d1[:, i:], float('inf'))\n",
    "#         np.fill_diagonal(d1[i:, :], float('inf'))\n",
    "#         np.fill_diagonal(d2[:, i:], float('inf'))\n",
    "#         np.fill_diagonal(d2[i:, :], float('inf'))\n",
    "\n",
    "#     # Calculate binary matrix of contacts\n",
    "#     contacts = np.abs(d1 - d2) < sigma\n",
    "#     return contacts.sum() / (length * length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdb_models(pdb_path):\n",
    "    \"\"\"\n",
    "    Load all models from a PDB file.\n",
    "    Returns a list of numpy arrays, each representing one model's coordinates (N, 3).\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    current_coords = []\n",
    "    in_model = False\n",
    "    \n",
    "    with open(pdb_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            rec = line[:6].strip().upper()\n",
    "            \n",
    "            if rec == \"MODEL\":\n",
    "                # If nested MODEL found without ENDMDL, flush previous if any\n",
    "                if in_model and current_coords:\n",
    "                    models.append(np.array(current_coords, dtype=np.float64))\n",
    "                    current_coords = []\n",
    "                in_model = True\n",
    "                continue\n",
    "                \n",
    "            if rec in (\"ATOM\", \"HETATM\") and in_model:\n",
    "                try:\n",
    "                    # PDB columns: x[30:38], y[38:46], z[46:54]\n",
    "                    x = float(line[30:38])\n",
    "                    y = float(line[38:46])\n",
    "                    z = float(line[46:54])\n",
    "                    current_coords.append([x, y, z])\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "                    \n",
    "            elif rec == \"ENDMDL\" and in_model:\n",
    "                if current_coords:\n",
    "                    models.append(np.array(current_coords, dtype=np.float64))\n",
    "                    current_coords = []\n",
    "                in_model = False\n",
    "    \n",
    "    # Handle files that end without ENDMDL\n",
    "    if in_model and current_coords:\n",
    "        models.append(np.array(current_coords, dtype=np.float64))\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def get_XYZ(files, file_type='auto'):\n",
    "    \"\"\"\n",
    "    Load coordinates from files (supports both PDB and GRO formats).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    files : list\n",
    "        List of file paths\n",
    "    file_type : str\n",
    "        'auto' (detect from extension), 'pdb', or 'gro'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List of coordinate arrays. For multi-model PDB files, expands to return \n",
    "    one array per model (so if file has 100 models, returns 100 arrays).\n",
    "    \"\"\"\n",
    "    all_xyz = []\n",
    "    \n",
    "    for file in files:\n",
    "        # Determine file type\n",
    "        if file_type == 'auto':\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            is_pdb = ext == '.pdb'\n",
    "        else:\n",
    "            is_pdb = file_type.lower() == 'pdb'\n",
    "        \n",
    "        if is_pdb:\n",
    "            # Load all models from PDB file\n",
    "            models = load_pdb_models(file)\n",
    "            # If file has multiple models, add each as a separate structure\n",
    "            if len(models) > 0:\n",
    "                all_xyz.extend(models)\n",
    "            else:\n",
    "                print(f\"Warning: No models found in {file}\")\n",
    "        else:\n",
    "            # Use MDTraj for GRO files (or other formats)\n",
    "            try:\n",
    "                traj = md.load(file)\n",
    "                if traj.n_frames > 0:\n",
    "                    # For GRO files, use first frame\n",
    "                    all_xyz.append(traj.xyz[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {file} with MDTraj: {e}\")\n",
    "    \n",
    "    return all_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reduced_structure(XYZ,patches):\n",
    "\n",
    "    current_patch_type = patches[0]\n",
    "    patch_indices = []\n",
    "\n",
    "    # This list will store the new reduced polymer structure\n",
    "    reduced_structure = []\n",
    "    reduced_sequence = []\n",
    "    for i, patch in enumerate(patches):\n",
    "        if patch == current_patch_type:\n",
    "            patch_indices.append(i)\n",
    "        if patch != current_patch_type or i == len(patches) - 1:  # Adding condition for the last element\n",
    "            # When the patch changes or we are at the end, calculate the COM of the current patch\n",
    "            COM = np.mean(XYZ[patch_indices, :], axis=0)\n",
    "            reduced_structure.append(COM)\n",
    "            reduced_sequence.append(current_patch_type)\n",
    "            \n",
    "            patch_indices = [i]\n",
    "            current_patch_type = patch\n",
    "\n",
    "    reduced_structure = np.array(reduced_structure)\n",
    "    return reduced_structure, reduced_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reduced_traj(XYZ_t,patches):\n",
    "    XYZ_t_reduced = []\n",
    "    for i in range(len(XYZ_t)):\n",
    "        m,a = calculate_reduced_structure(XYZ_t[i],patches)\n",
    "        XYZ_t_reduced.append(m)\n",
    "    return XYZ_t_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_parallel_all(args):\n",
    "    i, j, XYZ_MiChroM, XYZ_Full, XYZ_HP,beads,neig_short,neig_long,sigma = args\n",
    "    Q_M_M = compute_Q(XYZ_MiChroM[i], XYZ_MiChroM[j], beads,neig_short,neig_long,sigma)\n",
    "    Q_F_F = compute_Q(XYZ_Full[i], XYZ_Full[j], beads,neig_short,neig_long,sigma)\n",
    "    Q_M_F = compute_Q(XYZ_MiChroM[i], XYZ_Full[j], beads,neig_short,neig_long,sigma)\n",
    "    Q_H_H = compute_Q(XYZ_HP[i], XYZ_HP[j], beads,neig_short,neig_long,sigma)\n",
    "    Q_M_H = compute_Q(XYZ_MiChroM[i], XYZ_HP[j], beads,neig_short,neig_long,sigma)\n",
    "    Q_F_H = compute_Q(XYZ_Full[i], XYZ_HP[j], beads,neig_short,neig_long,sigma)\n",
    "    \n",
    "    return Q_M_M, Q_F_F, Q_M_F, Q_H_H, Q_M_H, Q_F_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_parallel(args):\n",
    "    i, j, XYZ_1,XYZ_2,beads = args\n",
    "    return compute_Q(XYZ_1[i], XYZ_2[j], beads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_triangle_to_square(arr):\n",
    "    \"\"\"\n",
    "    Convert a 1D array representing the upper triangle of a matrix \n",
    "    (including the diagonal) into a square matrix.\n",
    "    \"\"\"\n",
    "    n = int((-1 + np.sqrt(1 + 8 * len(arr))) // 2)\n",
    "    matrix = np.zeros((n, n))\n",
    "    matrix[np.triu_indices(n)] = arr\n",
    "    matrix += matrix.T - np.diag(matrix.diagonal())\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_square(arr):\n",
    "    \"\"\"\n",
    "    Convert a 1D array to a square matrix.\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(arr)))\n",
    "    if n*n != len(arr):\n",
    "        raise ValueError(\"Input array length is not a perfect square\")\n",
    "    return arr.reshape(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RG(xyz):\n",
    "    R\"\"\"\n",
    "    Calculates the Radius of Gyration of a chromosome chain.\n",
    "    \n",
    "    Returns:\n",
    "            Returns the Radius of Gyration in units of :math:`\\sigma`\n",
    "    \"\"\"\n",
    "    data = xyz\n",
    "    data = data - np.mean(data, axis=0)[None,:]\n",
    "    return np.sqrt(np.sum(np.var(np.array(data), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_gro_files(base_dir, X, seed=None):\n",
    "    \"\"\"\n",
    "    Select X random .gro files from the directory structure.\n",
    "\n",
    "    :param base_dir: The base directory where the folders are.\n",
    "    :param X: Number of random files to select.\n",
    "    :param seed: Seed for reproducibility.\n",
    "    :return: List of randomly selected .gro files.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Search for all .gro files in the numbered directories\n",
    "    all_files = glob.glob(f\"{base_dir}/*/*.gro\")\n",
    "    # Exclude files named run_M_0_block0.gro\n",
    "    files_MiChroM = [f for f in all_files if not f.endswith('run_M_0_block0.gro')]\n",
    "    # Randomly select X files\n",
    "    selected_files = random.sample(files_MiChroM, X)\n",
    "\n",
    "    return selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarsen(reduction, x, axes, trim_excess=False):\n",
    "    \"\"\"\n",
    "    Coarsen an array by applying reduction to fixed size neighborhoods.\n",
    "    Adapted from `dask.array.coarsen` to work on regular numpy arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : function\n",
    "        Function like np.sum, np.mean, etc...\n",
    "    x : np.ndarray\n",
    "        Array to be coarsened\n",
    "    axes : dict\n",
    "        Mapping of axis to coarsening factor\n",
    "    trim_excess : bool, optional\n",
    "        Remove excess elements. Default is False.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Provide dictionary of scale per dimension\n",
    "\n",
    "    >>> x = np.array([1, 2, 3, 4, 5, 6])\n",
    "    >>> coarsen(np.sum, x, {0: 2})\n",
    "    array([ 3,  7, 11])\n",
    "\n",
    "    >>> coarsen(np.max, x, {0: 3})\n",
    "    array([3, 6])\n",
    "\n",
    "    >>> x = np.arange(24).reshape((4, 6))\n",
    "    >>> x\n",
    "    array([[ 0,  1,  2,  3,  4,  5],\n",
    "           [ 6,  7,  8,  9, 10, 11],\n",
    "           [12, 13, 14, 15, 16, 17],\n",
    "           [18, 19, 20, 21, 22, 23]])\n",
    "\n",
    "    >>> coarsen(np.min, x, {0: 2, 1: 3})\n",
    "    array([[ 0,  3],\n",
    "           [12, 15]])\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    dask.array.coarsen\n",
    "\n",
    "    \"\"\"\n",
    "    # Insert singleton dimensions if they don't exist already\n",
    "    for i in range(x.ndim):\n",
    "        if i not in axes:\n",
    "            axes[i] = 1\n",
    "\n",
    "    if trim_excess:\n",
    "        ind = tuple(\n",
    "            slice(0, -(d % axes[i])) if d % axes[i] else slice(None, None)\n",
    "            for i, d in enumerate(x.shape)\n",
    "        )\n",
    "        x = x[ind]\n",
    "\n",
    "    # (10, 10) -> (5, 2, 5, 2)\n",
    "    newdims = [(x.shape[i] // axes[i], axes[i]) for i in range(x.ndim)]\n",
    "    newshape = tuple(np.concatenate(newdims))\n",
    "    reduction_axes = tuple(range(1, x.ndim * 2, 2))\n",
    "    return reduction(x.reshape(newshape), axis=reduction_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDB files directly\n",
    "# You can modify these paths to point to your PDB files\n",
    "pdb_file_1 = \"/Users/amk19/Desktop/ChromatinVAE/outputs_cluster/Generated_Samples/generated_samples_uniform.pdb\"\n",
    "pdb_file_2 = \"/Users/amk19/Desktop/ChromatinVAE/Data/chromosome21_aligned.pdb\"\n",
    "\n",
    "# Convert single PDB files to lists (get_XYZ expects a list of files)\n",
    "# For comparison, we'll treat each model in the PDB as a separate structure\n",
    "files_MiChroM = [pdb_file_1]  # Generated samples\n",
    "files_Full = [pdb_file_2]      # Reference data\n",
    "\n",
    "# If you want to use only a subset of models, you can load separately\n",
    "# For now, using all models from each file\n",
    "files_HP = []  # Not used in this comparison, but keeping for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_MiChroM = glob.glob(\"/mnt/d/Mammoth/Simulations/random_M_Run_M/*.gro\")\n",
    "# # files_Full = glob.glob(\"/mnt/d/Mammoth/Simulations/random_M_Run_53/*.gro\")\n",
    "# files_HP = glob.glob(\"/mnt/d/Mammoth/Simulations/random_M_Run_HP/*.gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_MiChroM),len(files_Full),len(files_HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_M_M=[]\n",
    "# Q_F_F=[]\n",
    "# Q_H_H=[]\n",
    "# Q_M_F=[]\n",
    "# Q_M_H=[]\n",
    "# Q_F_H=[]\n",
    "\n",
    "# for i,j in itertools.combinations_with_replacement(range(0,len(files_MiChroM)),2):\n",
    "#     print(i,j)\n",
    "#     Q_M_M.append(compute_Q(XYZ_MiChroM[i],XYZ_MiChroM[j],481,neig_short,neig_long,sigma))\n",
    "#     Q_F_F.append(compute_Q(XYZ_Full[i],XYZ_Full[j],481,neig_short,neig_long,sigma))\n",
    "#     Q_M_F.append(compute_Q(XYZ_MiChroM[i],XYZ_Full[j],481,neig_short,neig_long,sigma))\n",
    "#     Q_H_H.append(compute_Q(XYZ_HP[i],XYZ_HP[j],481,neig_short,neig_long,sigma))\n",
    "#     Q_M_H.append(compute_Q(XYZ_MiChroM[i],XYZ_HP[j],481,neig_short,neig_long,sigma))\n",
    "#     Q_F_H.append(compute_Q(XYZ_Full[i],XYZ_HP[j],481,neig_short,neig_long,sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 structures from generated samples\n",
      "Loaded 7591 structures from reference data\n",
      "Generated samples have 651 beads per structure\n",
      "Reference data has 651 beads per structure\n"
     ]
    }
   ],
   "source": [
    "# Load coordinates from PDB files\n",
    "# Each model in a PDB file will be loaded as a separate structure\n",
    "XYZ_MiChroM_all = get_XYZ(files_MiChroM, file_type='pdb')\n",
    "XYZ_Full_all = get_XYZ(files_Full, file_type='pdb')\n",
    "\n",
    "# Check if we have HP files, otherwise use empty list\n",
    "if len(files_HP) > 0:\n",
    "    XYZ_HP_all = get_XYZ(files_HP, file_type='pdb')\n",
    "else:\n",
    "    XYZ_HP_all = []\n",
    "\n",
    "print(f\"Loaded {len(XYZ_MiChroM_all)} structures from generated samples\")\n",
    "print(f\"Loaded {len(XYZ_Full_all)} structures from reference data\")\n",
    "if len(XYZ_HP_all) > 0:\n",
    "    print(f\"Loaded {len(XYZ_HP_all)} structures from HP data\")\n",
    "    \n",
    "# Check structure sizes\n",
    "if len(XYZ_MiChroM_all) > 0:\n",
    "    print(f\"Generated samples have {len(XYZ_MiChroM_all[0])} beads per structure\")\n",
    "if len(XYZ_Full_all) > 0:\n",
    "    print(f\"Reference data has {len(XYZ_Full_all[0])} beads per structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full structures without patch reduction\n"
     ]
    }
   ],
   "source": [
    "# Optional: Load patch information for structure reduction\n",
    "# If you don't have a patch file, you can skip this cell and use the full structures\n",
    "# Comment out the section below if you want to use full structures without reduction\n",
    "\n",
    "use_patch_reduction = False  # Set to True if you have a patch file\n",
    "\n",
    "if use_patch_reduction:\n",
    "    filename = \"/mnt/d/Mammoth/Simulations/M_Run_M/M_for_Training/chr_HiC_scaffold_23_250kb_eigen.seq\"\n",
    "    # Lists to store indices and patches\n",
    "    indices = []\n",
    "    patches = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split each line into index and patch\n",
    "            index, patch = line.split()\n",
    "            indices.append(int(index))\n",
    "            patches.append(patch)\n",
    "else:\n",
    "    patches = None\n",
    "    print(\"Using full structures without patch reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping reduced structure calculation - using full structures (patches is None)\n"
     ]
    }
   ],
   "source": [
    "# Only calculate reduced structure if patches are available\n",
    "if patches is not None:\n",
    "    reduced_structure,reduced_sequence = calculate_reduced_structure(XYZ_MiChroM_all[0],patches)\n",
    "    print(f\"Reduced structure shape: {reduced_structure.shape}, Reduced sequence length: {len(reduced_sequence)}\")\n",
    "else:\n",
    "    print(\"Skipping reduced structure calculation - using full structures (patches is None)\")\n",
    "    reduced_structure = None\n",
    "    reduced_sequence = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full structure shape: (651, 3)\n"
     ]
    }
   ],
   "source": [
    "# Only print reduced structure info if it was calculated\n",
    "if reduced_structure is not None:\n",
    "    print(f\"Reduced structure shape: {np.shape(reduced_structure)}, Reduced sequence length: {len(reduced_sequence)}\")\n",
    "else:\n",
    "    print(\"Full structure shape:\", np.shape(XYZ_MiChroM_all[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiChroM: 1000 structures\n",
      "Full: 7591 structures\n",
      "HP: 0 structures\n"
     ]
    }
   ],
   "source": [
    "## Apply patch reduction if patches are available, otherwise use full structures\n",
    "\n",
    "if patches is not None:\n",
    "    XYZ_MiChroM = compute_reduced_traj(XYZ_MiChroM_all, patches)\n",
    "    XYZ_Full = compute_reduced_traj(XYZ_Full_all, patches)\n",
    "    if len(XYZ_HP_all) > 0:\n",
    "        XYZ_HP = compute_reduced_traj(XYZ_HP_all, patches)\n",
    "    else:\n",
    "        XYZ_HP = []\n",
    "else:\n",
    "    # Use full structures without reduction\n",
    "    XYZ_MiChroM = XYZ_MiChroM_all\n",
    "    XYZ_Full = XYZ_Full_all\n",
    "    XYZ_HP = XYZ_HP_all\n",
    "    \n",
    "print(f\"MiChroM: {len(XYZ_MiChroM)} structures\")\n",
    "print(f\"Full: {len(XYZ_Full)} structures\")\n",
    "print(f\"HP: {len(XYZ_HP)} structures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(XYZ_HP),np.shape(XYZ_HP_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###All points\n",
    "\n",
    "# XYZ_MiChroM = XYZ_MiChroM_all\n",
    "# XYZ_Full = XYZ_Full_all\n",
    "# XYZ_HP = XYZ_HP_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot for averaged_structure\n",
    "# ax1 = fig.add_subplot(121, projection='3d')\n",
    "# ax1.scatter(XYZ_MiChroM[0][:, 0], XYZ_MiChroM[0][:, 1], XYZ_MiChroM[0][:, 2], marker='o')\n",
    "# ax1.set_title(\"Averaged Structure\")\n",
    "# ax1.set_xlabel(\"X\")\n",
    "# ax1.set_ylabel(\"Y\")\n",
    "# ax1.set_zlabel(\"Z\")\n",
    "\n",
    "# # Plot for reduced_structure\n",
    "# ax2 = fig.add_subplot(122, projection='3d')\n",
    "# ax2.scatter(reduced_structure[:, 0], reduced_structure[:, 1], reduced_structure[:, 2], marker='o')\n",
    "# ax2.set_title(\"Reduced Structure\")\n",
    "# ax2.set_xlabel(\"X\")\n",
    "# ax2.set_ylabel(\"Y\")\n",
    "# ax2.set_zlabel(\"Z\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of beads: 651\n",
      "Q parameters: sigma=2, neig_short=5, neig_long=200\n"
     ]
    }
   ],
   "source": [
    "# Determine number of beads (should be same for all structures in each group)\n",
    "if len(XYZ_MiChroM) > 0:\n",
    "    beads = len(XYZ_MiChroM[0])\n",
    "else:\n",
    "    beads = len(XYZ_Full[0]) if len(XYZ_Full) > 0 else 0\n",
    "\n",
    "sigma = 2\n",
    "neig_short = 5\n",
    "neig_long = 200\n",
    "\n",
    "print(f\"Number of beads: {beads}\")\n",
    "print(f\"Q parameters: sigma={sigma}, neig_short={neig_short}, neig_long={neig_long}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_cores=12\n",
    "# # pool = Pool(processes=n_cores)  #i, j, XYZ_MiChroM, XYZ_Full, XYZ_HP,beads,sigma,N = args\n",
    "# # results = pool.map(compute_Q_parallel_all, [(i, j, XYZ_MiChroM, XYZ_Full, XYZ_HP, beads,neig_short,neig_long,sigma) for i, j in itertools.combinations_with_replacement(range(len(files_MiChroM)), 2)])\n",
    "\n",
    "# n_cores = 12\n",
    "# pool = Pool(processes=n_cores)\n",
    "\n",
    "# # Combinations for cross dataset comparisons\n",
    "# cross_dataset_combinations = list(itertools.product(range(len(files_MiChroM)), repeat=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for worker functions (set before multiprocessing)\n",
    "_worker_XYZ_MiChroM = None\n",
    "_worker_XYZ_Full = None\n",
    "_worker_XYZ_HP = None\n",
    "_worker_beads = None\n",
    "_worker_neig_short = None\n",
    "_worker_neig_long = None\n",
    "_worker_sigma = None\n",
    "\n",
    "def init_worker(XYZ_MiChroM, XYZ_Full, XYZ_HP, beads, neig_short, neig_long, sigma):\n",
    "    \"\"\"Initialize worker process with shared data\"\"\"\n",
    "    global _worker_XYZ_MiChroM, _worker_XYZ_Full, _worker_XYZ_HP\n",
    "    global _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma\n",
    "    _worker_XYZ_MiChroM = XYZ_MiChroM\n",
    "    _worker_XYZ_Full = XYZ_Full\n",
    "    _worker_XYZ_HP = XYZ_HP\n",
    "    _worker_beads = beads\n",
    "    _worker_neig_short = neig_short\n",
    "    _worker_neig_long = neig_long\n",
    "    _worker_sigma = sigma\n",
    "\n",
    "def compute_Q_parallel_same(args):\n",
    "    \"\"\"Compute Q for same-dataset comparisons - takes only indices\"\"\"\n",
    "    i, j = args\n",
    "    Q_M_M = compute_Q(_worker_XYZ_MiChroM[i], _worker_XYZ_MiChroM[j], \n",
    "                      _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "    Q_F_F = compute_Q(_worker_XYZ_Full[i], _worker_XYZ_Full[j], \n",
    "                      _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "    \n",
    "    # Only compute HP comparisons if HP dataset exists and indices are valid\n",
    "    if len(_worker_XYZ_HP) > 0 and i < len(_worker_XYZ_HP) and j < len(_worker_XYZ_HP):\n",
    "        Q_H_H = compute_Q(_worker_XYZ_HP[i], _worker_XYZ_HP[j], \n",
    "                         _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "    else:\n",
    "        Q_H_H = np.nan\n",
    "    \n",
    "    return Q_M_M, Q_F_F, Q_H_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_parallel_cross(args):\n",
    "    \"\"\"Compute Q for cross-dataset comparisons - takes only indices\"\"\"\n",
    "    i, j = args\n",
    "    Q_M_F = compute_Q(_worker_XYZ_MiChroM[i], _worker_XYZ_Full[j], \n",
    "                     _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "    \n",
    "    # Only compute HP comparisons if HP dataset exists and indices are valid\n",
    "    if len(_worker_XYZ_HP) > 0 and i < len(_worker_XYZ_HP) and j < len(_worker_XYZ_HP):\n",
    "        Q_M_H = compute_Q(_worker_XYZ_MiChroM[i], _worker_XYZ_HP[j], \n",
    "                         _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "        Q_F_H = compute_Q(_worker_XYZ_Full[i], _worker_XYZ_HP[j], \n",
    "                         _worker_beads, _worker_neig_short, _worker_neig_long, _worker_sigma)\n",
    "    else:\n",
    "        Q_M_H = np.nan\n",
    "        Q_F_H = np.nan\n",
    "    \n",
    "    return Q_M_F, Q_M_H, Q_F_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_triangle_to_square_without_diagonal(arr):\n",
    "    n = int(np.sqrt(2*len(arr) + 0.25) - 0.5)  # New formula to calculate size\n",
    "    matrix = np.zeros((n, n))\n",
    "    indices = np.triu_indices(n, 1)  # Exclude diagonal\n",
    "    matrix[indices] = arr\n",
    "    matrix += matrix.T\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Q values for 1000 MiChroM, 7591 Full, 0 HP structures\n",
      "Created 500500 same-dataset tasks and 1000000 cross-dataset tasks\n",
      "Optimized: tasks now only contain indices (much faster!)\n",
      "Initializing worker processes with shared data...\n",
      "Computing same-dataset comparisons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Same-dataset Q values:  34%|███▍      | 170013/500500 [05:43<12:06, 454.61it/s]"
     ]
    }
   ],
   "source": [
    "n_cores=60\n",
    "\n",
    "# Calculate Q values - using actual number of structures loaded (not number of files)\n",
    "n_structures_M = len(XYZ_MiChroM)\n",
    "n_structures_F = len(XYZ_Full)\n",
    "n_structures_H = len(XYZ_HP)\n",
    "\n",
    "print(f\"Computing Q values for {n_structures_M} MiChroM, {n_structures_F} Full, {n_structures_H} HP structures\")\n",
    "\n",
    "# Prepare tasks - now ONLY indices (much faster and smaller memory footprint)\n",
    "# For same-dataset comparisons (within each dataset)\n",
    "same_tasks = [(i, j) for i, j in itertools.combinations_with_replacement(range(n_structures_M), 2)]\n",
    "\n",
    "# For cross-dataset comparisons (between datasets)\n",
    "if n_structures_H > 0:\n",
    "    cross_combinations = list(itertools.product(range(min(n_structures_M, n_structures_F, n_structures_H)), repeat=2))\n",
    "else:\n",
    "    cross_combinations = list(itertools.product(range(min(n_structures_M, n_structures_F)), repeat=2))\n",
    "\n",
    "cross_tasks = [(i, j) for i, j in cross_combinations]\n",
    "\n",
    "print(f\"Created {len(same_tasks)} same-dataset tasks and {len(cross_tasks)} cross-dataset tasks\")\n",
    "print(f\"Optimized: tasks now only contain indices (much faster!)\")\n",
    "\n",
    "# For multiprocessing on macOS with spawn, use initializer to pass arrays once\n",
    "try:\n",
    "    # Create Pool with initializer to pass arrays only once per worker\n",
    "    print(\"Initializing worker processes with shared data...\")\n",
    "    pool = Pool(processes=n_cores, \n",
    "                initializer=init_worker, \n",
    "                initargs=(XYZ_MiChroM, XYZ_Full, XYZ_HP, beads, neig_short, neig_long, sigma))\n",
    "    \n",
    "    # Use imap for progress tracking with multiprocessing\n",
    "    print(\"Computing same-dataset comparisons...\")\n",
    "    results_same = list(tqdm(pool.imap(compute_Q_parallel_same, same_tasks, chunksize=10), \n",
    "                            total=len(same_tasks), \n",
    "                            desc=\"Same-dataset Q values\"))\n",
    "    \n",
    "    print(\"Computing cross-dataset comparisons...\")\n",
    "    results_cross = list(tqdm(pool.imap(compute_Q_parallel_cross, cross_tasks, chunksize=10), \n",
    "                             total=len(cross_tasks), \n",
    "                             desc=\"Cross-dataset Q values\"))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Completed parallel Q calculations using multiprocessing\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Multiprocessing failed: {e}\")\n",
    "    print(\"Falling back to serial computation (this will be slower)...\")\n",
    "    \n",
    "    # Serial fallback - initialize worker globals first\n",
    "    init_worker(XYZ_MiChroM, XYZ_Full, XYZ_HP, beads, neig_short, neig_long, sigma)\n",
    "    \n",
    "    # Serial computation with progress bars\n",
    "    print(f\"Processing {len(same_tasks)} same-dataset comparisons...\")\n",
    "    results_same = [compute_Q_parallel_same(task) for task in tqdm(same_tasks, desc=\"Same-dataset Q values\")]\n",
    "    \n",
    "    print(f\"Processing {len(cross_tasks)} cross-dataset comparisons...\")\n",
    "    results_cross = [compute_Q_parallel_cross(task) for task in tqdm(cross_tasks, desc=\"Cross-dataset Q values\")]\n",
    "    \n",
    "    print(\"Completed Q calculations using serial computation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack results\n",
    "Q_M_M, Q_F_F, Q_H_H = zip(*results_same)\n",
    "Q_M_F, Q_M_H, Q_F_H = zip(*results_cross)\n",
    "\n",
    "# Convert to arrays and handle NaN values if HP dataset is empty\n",
    "Q_M_M = np.array(Q_M_M)\n",
    "Q_F_F = np.array(Q_F_F)\n",
    "Q_H_H = np.array(Q_H_H)\n",
    "Q_M_F = np.array(Q_M_F)\n",
    "Q_M_H = np.array(Q_M_H)\n",
    "Q_F_H = np.array(Q_F_H)\n",
    "\n",
    "# Filter out NaN values if HP dataset was empty\n",
    "if len(XYZ_HP) == 0:\n",
    "    print(\"Note: HP dataset is empty - filtering out NaN values\")\n",
    "    # Filter Q_H_H (same dataset comparisons)\n",
    "    Q_H_H = Q_H_H[~np.isnan(Q_H_H)]\n",
    "    # Filter Q_M_H and Q_F_H (cross dataset comparisons)\n",
    "    Q_M_H = Q_M_H[~np.isnan(Q_M_H)]\n",
    "    Q_F_H = Q_F_H[~np.isnan(Q_F_H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rg_M_M=[]\n",
    "# for i in range(len(XYZ_MiChroM)):\n",
    "#     Rg_M_M.append(compute_RG(XYZ_MiChroM[i]))\n",
    "\n",
    "# Rg_F_F=[]\n",
    "# for i in range(len(XYZ_Full)):\n",
    "#     Rg_F_F.append(compute_RG(XYZ_Full[i]))\n",
    "\n",
    "# Rg_H_H=[]\n",
    "# for i in range(len(XYZ_HP)):\n",
    "#     Rg_H_H.append(compute_RG(XYZ_HP[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Q_F_H) - Q_M_M_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_cores=5\n",
    "# pool = Pool(processes=n_cores)\n",
    "# Q_M_M = pool.map(compute_Q_parallel, [(i, j, XYZ_MiChroM, XYZ_MiChroM,beads) for i, j in itertools.combinations_with_replacement(range(len(files_MiChroM)), 2)])\n",
    "# Q_M_H = pool.map(compute_Q_parallel, [(i, j, XYZ_MiChroM, XYZ_HP,beads) for i, j in itertools.combinations_with_replacement(range(len(files_MiChroM)), 2)])\n",
    "# Q_H_H = pool.map(compute_Q_parallel, [(i, j, XYZ_HP, XYZ_HP,beads) for i, j in itertools.combinations_with_replacement(range(len(files_MiChroM)), 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=len(Q_M_F)\n",
    "# l=int((-1+np.sqrt(1+8*N))/2)\n",
    "\n",
    "# print(l,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_Q_M_F= np.zeros((l,l))\n",
    "# indices = np.triu_indices(l)\n",
    "# M_Q_M_F[indices] = Q_M_F[0:N]\n",
    "\n",
    "# matrix_all = M_Q_M_F + M_Q_M_F.T - np.diag(M_Q_M_F.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_M_M = np.array(Q_M_M)\n",
    "Q_F_F = np.array(Q_F_F)\n",
    "Q_H_H = np.array(Q_H_H)\n",
    "Q_M_F = np.array(Q_M_F)\n",
    "Q_M_H = np.array(Q_M_H)\n",
    "Q_F_H = np.array(Q_F_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q_M_F),len(Q_M_M),len(Q_F_F),len(Q_H_H),len(Q_M_H),len(Q_F_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_M_M_matrix = upper_triangle_to_square(Q_M_M)\n",
    "Q_F_F_matrix = upper_triangle_to_square(Q_F_F)\n",
    "Q_H_H_matrix = upper_triangle_to_square(Q_H_H)\n",
    "\n",
    "Q_M_F_matrix = array_to_square(Q_M_F)\n",
    "Q_M_H_matrix = array_to_square(Q_M_H)\n",
    "Q_F_H_matrix = array_to_square(Q_F_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix = np.block([\n",
    "    [Q_M_M_matrix, Q_M_F_matrix, Q_M_H_matrix],\n",
    "    [Q_M_F_matrix.T, Q_F_F_matrix, Q_F_H_matrix],\n",
    "    [Q_M_H_matrix.T, Q_F_H_matrix.T, Q_H_H_matrix]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/vinicius/Work/Mammoth/Simulations/Q_Matrix.npy',merged_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix = np.block([\n",
    "#     [Q_M_M_matrix, Q_M_H_matrix],\n",
    "#     [Q_M_H_matrix.T, Q_H_H_matrix]\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_removed = matrix_all[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_M_M_matrix[8][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmin=0\n",
    "# vmax=0.1\n",
    "\n",
    "# # plt.imshow(Q_M_M_matrix[0:-1][:,0:-1], interpolation='None', cmap='viridis_r', vmin=vmin, vmax=vmax)\n",
    "\n",
    "# # plt.imshow(Q_F_F_matrix[0:-1][:,0:-1], interpolation='None', cmap='viridis_r', vmin=vmin, vmax=vmax)\n",
    "\n",
    "# plt.imshow(Q_H_H_matrix[0:-1][:,0:-1], interpolation='None', cmap='viridis_r', vmin=vmin, vmax=vmax)\n",
    "\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = sns.choose_cubehelix_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin=0.2\n",
    "vmax=0.8\n",
    "# palette = cubehelix.classic_16.mpl_colormap\n",
    "# palette = palettable.mycarta.LinearL_4.mpl_colormap\n",
    "# palette = palettable.lightbartlein.diverging.BlueDarkRed18_9.mpl_colormap\n",
    "# palette = palettable.scientific.sequential.Acton_3_r.mpl_colormap\n",
    "\n",
    "palette = palettable.cmocean.sequential.Ice_3_r.mpl_colormap\n",
    "\n",
    "plt.imshow(merged_matrix, interpolation='None', cmap=palette, vmin=vmin, vmax=vmax)\n",
    "# plt.savefig('/home/vinicius/Work/Mammoth/Simulations/Q_Matrix_test.jpg',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = 1 - merged_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t=1.3 # sim 47\n",
    "# # t = 4.2 # sim 1000\n",
    "# # t= 1.4 # exp\n",
    "\n",
    "# t=1.3\n",
    "\n",
    "# Z = linkage(dissimilarity_matrix, method='complete')\n",
    "# T = fcluster(Z, t=t, criterion='distance')\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# d = dendrogram(Z, color_threshold=t)\n",
    "# # d = dendrogram(Z)\n",
    "\n",
    "# plt.savefig('/home/vinicius/Work/Mammoth/Simulations/Dendogram_test1.jpg',dpi=300)\n",
    "# # plt.xticks([])\n",
    "# # plt.savefig('/mnt/d/DT40/Centromere-Modeling/Data_Gro/Figures/Dendogram_3D_Images/Dendogram_sim_chr5_1000_set1.jpg',dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.cluster import hierarchy\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load precomputed dissimilarity data\n",
    "# dissimilarity = dissimilarity_matrix\n",
    "\n",
    "# # Compute linkage matrix using the dissimilarity data\n",
    "# Z = hierarchy.linkage(dissimilarity, method='complete')\n",
    "\n",
    "# # Compute optimal ordering of the rows and columns based on dendrogram linkage\n",
    "# optimal_order = hierarchy.leaves_list(Z)\n",
    "\n",
    "# # Reorder dissimilarity matrix and labels based on optimal order\n",
    "# reordered_dissimilarity = dissimilarity[optimal_order, :]\n",
    "# reordered_dissimilarity = reordered_dissimilarity[:, optimal_order]\n",
    "\n",
    "# # Reordered labels\n",
    "# reordered_labels = [i for i in optimal_order]\n",
    "\n",
    "# # Plot dendrogram and matrix plot together\n",
    "# fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 8), gridspec_kw={'width_ratios': [0.25, 0.75]})\n",
    "\n",
    "# # Plot dendrogram on the left\n",
    "\n",
    "# hierarchy.dendrogram(Z, ax=ax1, orientation='left', color_threshold=t)\n",
    "# ax1.invert_yaxis()  # Reverse the y-axis of the dendrogram\n",
    "# # ax1.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "\n",
    "# ax1.set_yticks([])\n",
    "# ax1.set_yticklabels([])\n",
    "\n",
    "# # Plot heatmap on the right\n",
    "# sns.heatmap(reordered_dissimilarity, ax=ax2, cmap=\"YlGnBu\", vmin=0,vmax=1, cbar=False)\n",
    "\n",
    "# # ax2.set_xticks([i + 0.5 for i in range(len(reordered_labels))])\n",
    "# # ax2.set_yticks([i + 0.5 for i in range(len(reordered_labels))])\n",
    "# # ax2.set_xticklabels(reordered_labels, fontsize=8)\n",
    "# # ax2.set_yticklabels(reordered_labels, fontsize=8, rotation=0)\n",
    "\n",
    "\n",
    "# # Remove x-axis and y-axis ticks and labels\n",
    "# ax2.set_xticks([])\n",
    "# ax2.set_xticklabels([])\n",
    "# ax2.set_yticks([])\n",
    "# ax2.set_yticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "# # Add title and axis labels\n",
    "# ax1.set_title(\"Dendrogram\", fontsize=14)\n",
    "# ax2.set_title(\"Dissimilarity Matrix\", fontsize=14)\n",
    "# ax2.set_xlabel(\"Structures\", fontsize=12)\n",
    "# ax2.set_ylabel(\"Structures\", fontsize=12)\n",
    "\n",
    "# # Adjust spacing between plots\n",
    "# fig.tight_layout(pad=2.0)\n",
    "\n",
    "# # Show plot\n",
    "\n",
    "# # plt.savefig('/mnt/d/DT40/Centromere-Modeling/Data_Gro/Figures/Dendogram_3D_Images/Q_Matrix_Dendogram_sim_chr5_1000_set1.jpg',dpi=600)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the dissimilarity matrix to a 1D array\n",
    "# flattened_matrix = dissimilarity_matrix.flatten()\n",
    "\n",
    "# # Plotting the histogram using Seaborn\n",
    "# sns.histplot(flattened_matrix, kde=False)\n",
    "# plt.xlabel('Dissimilarity Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Dissimilarity Matrix')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q_M_M),len(Q_F_F),len(Q_M_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the matrix into a 1D array\n",
    "# dissimilarity_data = dissimilarity_matrix.flatten()\n",
    "\n",
    "# # Remove redundant elements if it's a symmetric matrix\n",
    "# if np.allclose(dissimilarity_matrix, dissimilarity_matrix.T):\n",
    "#     dissimilarity_data = dissimilarity_data[dissimilarity_data > 0]\n",
    "\n",
    "# # Convert to a pandas Series\n",
    "# dissimilarity_series = pd.Series(dissimilarity_data)\n",
    "\n",
    "# # Create the density plot\n",
    "# sns.kdeplot(dissimilarity_series,fill=True,)\n",
    "\n",
    "# plt.xlabel('Dissimilarity Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Dissimilarity')\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "# data_save_folder = Path(\"/mnt/d/Mammoth/Images/Full-Inversion/\")\n",
    "# # file_plot = data_save_folder /  'histogram_Dissimilarity.jpeg'\n",
    "\n",
    "# # plt.savefig(file_plot,bbox_inches='tight',dpi=600)\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_M_M = np.array(Q_M_M)\n",
    "Q_F_F = np.array(Q_F_F)\n",
    "Q_H_H = np.array(Q_H_H)\n",
    "Q_M_F = np.array(Q_M_F)\n",
    "Q_M_H = np.array(Q_M_H)\n",
    "Q_F_H = np.array(Q_F_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q_M_M[Q_M_M != 1.0]),len(Q_F_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = {\n",
    "#     'MiChroM_vs_MiChroM': Q_M_M[Q_M_M != 1.0],\n",
    "#     'DirectInversion_vs_DirectInversion': Q_F_F[Q_F_F != 1.0],\n",
    "#     'Homopolymer_vs_Homopolymer':Q_H_H[Q_H_H != 1.0] ,\n",
    "# }\n",
    "\n",
    "# data2 = {\n",
    "#     'MiChroM_vs_DirectInversion':Q_M_F[Q_M_F != 1.0],\n",
    "#     'MiChroM_vs_Homopolymer': Q_M_H[Q_M_H != 1.0],\n",
    "#     'DirectInversion_vs_Homopolymer': Q_F_H[Q_F_H != 1.0],\n",
    "# }\n",
    "\n",
    "\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# df2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_M_M = pd.DataFrame({'Q': Q_M_M[Q_M_M != 1.0], 'Type': 'MiChroM_vs_MiChroM'})\n",
    "# df_F_F = pd.DataFrame({'Q': Q_F_F[Q_F_F != 1.0], 'Type': 'DirectInversion_vs_DirectInversion'})\n",
    "# df_H_H = pd.DataFrame({'Q': Q_H_H[Q_H_H != 1.0], 'Type': 'Homopolymer_vs_Homopolymer'})\n",
    "# df_M_F = pd.DataFrame({'Q': Q_M_F[Q_M_F != 1.0], 'Type': 'MiChroM_vs_DirectInversion'})\n",
    "# df_M_H = pd.DataFrame({'Q': Q_M_H[Q_M_H != 1.0], 'Type': 'MiChroM_vs_Homopolymer'})\n",
    "# df_F_H = pd.DataFrame({'Q': Q_F_H[Q_F_H != 1.0], 'Type': 'DirectInversion_vs_Homopolymer'})\n",
    "\n",
    "\n",
    "# Create DataFrames, only including non-empty arrays\n",
    "dataframes_list = []\n",
    "\n",
    "if len(Q_M_M) > 0:\n",
    "    df_M_M = pd.DataFrame({'Q': Q_M_M, 'Type': 'MiChroM_vs_MiChroM'})\n",
    "    dataframes_list.append(df_M_M)\n",
    "else:\n",
    "    df_M_M = pd.DataFrame({'Q': [], 'Type': []})\n",
    "\n",
    "if len(Q_F_F) > 0:\n",
    "    df_F_F = pd.DataFrame({'Q': Q_F_F, 'Type': 'DirectInversion_vs_DirectInversion'})\n",
    "    dataframes_list.append(df_F_F)\n",
    "else:\n",
    "    df_F_F = pd.DataFrame({'Q': [], 'Type': []})\n",
    "\n",
    "if len(Q_H_H) > 0:\n",
    "    df_H_H = pd.DataFrame({'Q': Q_H_H, 'Type': 'Homopolymer_vs_Homopolymer'})\n",
    "    dataframes_list.append(df_H_H)\n",
    "else:\n",
    "    df_H_H = pd.DataFrame({'Q': [], 'Type': []})\n",
    "\n",
    "if len(Q_M_F) > 0:\n",
    "    df_M_F = pd.DataFrame({'Q': Q_M_F, 'Type': 'MiChroM_vs_DirectInversion'})\n",
    "    dataframes_list.append(df_M_F)\n",
    "else:\n",
    "    df_M_F = pd.DataFrame({'Q': [], 'Type': []})\n",
    "\n",
    "if len(Q_M_H) > 0:\n",
    "    df_M_H = pd.DataFrame({'Q': Q_M_H, 'Type': 'MiChroM_vs_Homopolymer'})\n",
    "    dataframes_list.append(df_M_H)\n",
    "else:\n",
    "    df_M_H = pd.DataFrame({'Q': [], 'Type': []})\n",
    "\n",
    "if len(Q_F_H) > 0:\n",
    "    df_F_H = pd.DataFrame({'Q': Q_F_H, 'Type': 'DirectInversion_vs_Homopolymer'})\n",
    "    dataframes_list.append(df_F_H)\n",
    "else:\n",
    "    df_F_H = pd.DataFrame({'Q': [], 'Type': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only concatenate non-empty DataFrames\n",
    "dfs_to_concat = [df for df in [df_M_M, df_F_F, df_H_H, df_M_F, df_M_H, df_F_H] if len(df) > 0]\n",
    "df_combined = pd.concat(dfs_to_concat, ignore_index=True) if len(dfs_to_concat) > 0 else pd.DataFrame({'Q': [], 'Type': []})\n",
    "# df_combined = pd.concat([df_F_F,df_M_M, df_M_F, df_F_H, df_M_H,df_H_H], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Q_M_M),np.mean(Q_F_F),np.mean(Q_H_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for column in df1.columns:\n",
    "#     sns.kdeplot(df1[column], label=column)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for column in df2.columns:\n",
    "#     sns.kdeplot(df2[column], label=column)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df_combined, x='Q', hue='Type', fill=True, legend=True)\n",
    "plt.title('Distribution of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_types = ['Homopolymer_vs_Homopolymer', 'MiChroM_vs_DirectInversion', 'MiChroM_vs_MiChroM','DirectInversion_vs_DirectInversion']\n",
    "# selected_types = ['Homopolymer_vs_Homopolymer', 'MiChroM_vs_DirectInversion']\n",
    "# df_filtered = df_combined[df_combined['Type'].isin(selected_types)]\n",
    "\n",
    "df_filtered = df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epsilon = 0.001 # a small value\n",
    "\n",
    "\n",
    "# min_val = df_filtered['Q'].quantile(0.0005) + epsilon\n",
    "min_val = df_combined['Q'].min() - epsilon\n",
    "max_val = df_combined['Q'].quantile(0.9995) #- 0.1 #epsilon\n",
    "\n",
    "df_combined.loc[:, 'Normalized_Q'] = (df_combined['Q'] - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_combined[df_combined['Type'] == 'MiChroM_vs_MiChroM']['Normalized_Q']))\n",
    "print(len(df_combined[df_combined['Type'] == 'DirectInversion_vs_DirectInversion']['Normalized_Q']))\n",
    "print(len(df_combined[df_combined['Type'] == 'Homopolymer_vs_Homopolymer']['Normalized_Q']))\n",
    "print(len(df_combined[df_combined['Type'] == 'MiChroM_vs_DirectInversion']['Normalized_Q']))\n",
    "print(len(df_combined[df_combined['Type'] == 'MiChroM_vs_Homopolymer']['Normalized_Q']))\n",
    "print(len(df_combined[df_combined['Type'] == 'DirectInversion_vs_Homopolymer']['Normalized_Q']))\n",
    "\n",
    "def array_to_square(arr):\n",
    "    \"\"\"\n",
    "    Convert a 1D array to a 2D square matrix.\n",
    "    \"\"\"\n",
    "    arr_numpy = arr.to_numpy()  # Convert the Series to numpy array\n",
    "    n = int(np.sqrt(len(arr_numpy)))\n",
    "    if n*n != len(arr_numpy):\n",
    "        raise ValueError(\"Input array length is not a perfect square\")\n",
    "    return arr_numpy.reshape(n, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_M_M_matrix_combined = upper_triangle_to_square(df_combined[df_combined['Type'] == 'MiChroM_vs_MiChroM']['Normalized_Q'])\n",
    "Q_F_F_matrix_combined = upper_triangle_to_square(df_combined[df_combined['Type'] == 'DirectInversion_vs_DirectInversion']['Normalized_Q'])\n",
    "Q_M_F_matrix_combined = array_to_square(df_combined[df_combined['Type'] == 'MiChroM_vs_DirectInversion']['Normalized_Q'])\n",
    "Q_H_H_matrix_combined = upper_triangle_to_square(df_combined[df_combined['Type'] == 'Homopolymer_vs_Homopolymer']['Normalized_Q'])\n",
    "Q_M_H_matrix_combined = array_to_square(df_combined[df_combined['Type'] == 'MiChroM_vs_Homopolymer']['Normalized_Q'])\n",
    "Q_F_H_matrix_combined = array_to_square(df_combined[df_combined['Type'] == 'DirectInversion_vs_Homopolymer']['Normalized_Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix_combined = np.block([\n",
    "    [1.2*Q_M_M_matrix_combined, 1.1*Q_M_F_matrix_combined, Q_M_H_matrix_combined],\n",
    "    [1.1*Q_M_F_matrix_combined.T, Q_F_F_matrix_combined, Q_F_H_matrix_combined],\n",
    "    [Q_M_H_matrix_combined.T, Q_F_H_matrix_combined.T, 1.05*Q_H_H_matrix_combined]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_matrix_combined_C = coarsen(np.mean,merged_matrix_combined,{0: 1, 1: 1}, trim_excess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5),frameon=False)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "\n",
    "vmin=0.0\n",
    "vmax=1.0\n",
    "\n",
    "\n",
    "# palette = cubehelix.classic_16.mpl_colormap\n",
    "# palette = palettable.mycarta.LinearL_4.mpl_colormap\n",
    "# palette = palettable.lightbartlein.diverging.BlueDarkRed18_9.mpl_colormap\n",
    "# palette = palettable.scientific.sequential.Acton_3_r.mpl_colormap\n",
    "\n",
    "palette = palettable.cmocean.sequential.Ice_3_r.mpl_colormap\n",
    "\n",
    "cax = ax.imshow(merged_matrix_combined_C, interpolation='None', cmap=palette, vmin=vmin, vmax=vmax)\n",
    "\n",
    "plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "# plt.savefig('/home/vinicius/Work/Mammoth/Simulations/Q_Matrix_color.pdf',dpi=600, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import palettable\n",
    "\n",
    "\n",
    "# Extract key colors from the palettable colormap\n",
    "palette = palettable.cmocean.sequential.Ice_3_r.mpl_colormap\n",
    "start_color = palette(0.0)    # Color at the start of the colormap\n",
    "middle_color = palette(0.4) # Color at the middle of the colormap\n",
    "end_color = palette(0.99)      # Color at the end of the colormap\n",
    "\n",
    "# Define custom color transitions\n",
    "vmin = 0.25\n",
    "vmax = 0.8\n",
    "norm = mpl.colors.Normalize(vmin, vmax)\n",
    "colors = [\n",
    "    [norm(vmin), start_color],\n",
    "    [norm(0.4), middle_color], # Transition starts at 0.4\n",
    "    [norm(0.4), middle_color],    # Rapid transition after 0.4\n",
    "    [norm(vmax), end_color]\n",
    "]\n",
    "\n",
    "# Create a custom colormap\n",
    "custom_colormap = mpl.colors.LinearSegmentedColormap.from_list(\"custom_colormap\", colors)\n",
    "\n",
    "# Your plotting code\n",
    "fig = plt.figure(figsize=(5,5), frameon=False)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "\n",
    "# Use the new colormap\n",
    "cax = ax.imshow(merged_matrix_combined_C, interpolation='bicubic', cmap=custom_colormap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "# plt.savefig('/home/vinicius/Work/Mammoth/Simulations/Q_Matrix_color2.pdf', dpi=600, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 0.0  # a small value\n",
    "# min_val = df_filtered['Q'].min() + epsilon\n",
    "# max_val = df_filtered['Q'].max() - epsilon\n",
    "\n",
    "# df_filtered.loc[:, 'Normalized_Q'] = (df_filtered['Q'] - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "epsilon = 0.001 # a small value\n",
    "# type_specific_df_max = df_filtered[df_filtered['Type'] == 'MiChroM_vs_MiChroM']\n",
    "# type_specific_df_min = df_filtered[df_filtered['Type'] == 'Homopolymer_vs_Homopolymer']\n",
    "# min_val = #type_specific_df_min['Q'].min() + epsilon\n",
    "# max_val = 0.65 #type_specific_df_max['Q'].max() - epsilon\n",
    "\n",
    "# min_val = df_filtered['Q'].quantile(0.0005) + epsilon\n",
    "min_val = df_filtered['Q'].min() - epsilon\n",
    "max_val = df_filtered['Q'].quantile(0.9995) #- 0.1 #epsilon\n",
    "\n",
    "df_filtered.loc[:, 'Normalized_Q'] = (df_filtered['Q'] - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val,max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the data\n",
    "# selected_types = ['MiChroM_vs_DirectInversion', 'Homopolymer_vs_Homopolymer']\n",
    "# df_two_types = df_filtered[df_filtered['Type'].isin(selected_types)]\n",
    "\n",
    "# # Set the default font size for various elements\n",
    "# plt.rcParams['axes.labelsize'] = 30\n",
    "# plt.rcParams['axes.titlesize'] = 30\n",
    "# plt.rcParams['xtick.labelsize'] = 20\n",
    "# plt.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "# # Plot size\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Colors\n",
    "# colors = ['#808080','#007A1D']\n",
    "\n",
    "# # Plot\n",
    "# sns.kdeplot(data=df_two_types, x='Normalized_Q', hue='Type', fill=True, palette=colors, alpha=.75, linewidth=0, common_norm=False, legend=False, bw_adjust=1.5)\n",
    "\n",
    "# custom_labels = ['MiChroM+Full', 'Homopolymer']\n",
    "# plt.legend(labels=custom_labels, loc='upper left', frameon=False, fontsize=23)\n",
    "\n",
    "# # Labels\n",
    "# # plt.ylabel(r\"Probability Density [PD$(Q_{\\mathrm{norm}}$)]\")\n",
    "# # plt.xlabel(r'Similarity [$Q_{\\mathrm{norm}}$]')\n",
    "\n",
    "# plt.ylabel(r\"Probability Density [PD(Q)]\")\n",
    "# plt.xlabel(r'Similarity [Q]')\n",
    "\n",
    "# # Limit\n",
    "# plt.xlim(0.0, 1.0)\n",
    "\n",
    "\n",
    "\n",
    "# # Save and Show\n",
    "# plt.savefig('/mnt/d/Mammoth/Data_Final/PQ_Qnorm_test.jpg', dpi=600)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = palettable.cmocean.sequential.Speed_7_r.mpl_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(value):\n",
    "    value = value.lstrip('#')\n",
    "    length = len(value)\n",
    "    return tuple(int(value[i:i+length//3], 16)/255.0 for i in range(0, length, length//3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = colors[1:-1]\n",
    "# Add the gray color at the beginning\n",
    "gray_rgb = hex_to_rgb(\"#808080\")\n",
    "colors.append(gray_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.patches as patches\n",
    "# fig, ax = plt.subplots(1, figsize=(5, len(colors)))\n",
    "\n",
    "# for idx, color in enumerate(colors):\n",
    "#     ax.add_patch(patches.Rectangle((0, idx), 1, 1, facecolor=color))\n",
    "\n",
    "# ax.set_xlim(0, 1)\n",
    "# ax.set_ylim(0, len(colors))\n",
    "# ax.axis('off')  # Hide the axes\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "order = ['DirectInversion_vs_DirectInversion','MiChroM_vs_MiChroM','MiChroM_vs_DirectInversion','DirectInversion_vs_Homopolymer','MiChroM_vs_Homopolymer','Homopolymer_vs_Homopolymer']\n",
    "# colors = ['#291E8F', '#AD9C00','#007A1D','#808080']\n",
    "color_dict = dict(zip(df_filtered['Type'].unique(), colors))\n",
    "ordered_colors = [color_dict[item] for item in order]\n",
    "\n",
    "# Modify the height for larger plots\n",
    "height_value = 1.5\n",
    "# g = sns.FacetGrid(df_filtered, row=\"Type\", hue=\"Type\", aspect=5, height=height_value, palette=ordered_colors, row_order=order)\n",
    "\n",
    "g = sns.FacetGrid(df_filtered, row=\"Type\", hue=\"Type\", aspect=5, height=height_value, palette=colors, row_order=order)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"Normalized_Q\",\n",
    "      bw_adjust=1.5, clip_on=False,\n",
    "      fill=True, alpha=0.75, linewidth=0,common_norm=False, legend=False)\n",
    "g.map(sns.kdeplot, \"Normalized_Q\", clip_on=False, color=\"w\", lw=2, bw_adjust=1.5)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "# def label(x, color, label):\n",
    "#     ax = plt.gca()\n",
    "#     ax.text(0, .2, label, fontweight=\"normal\", color=color, fontsize=23,\n",
    "#             ha=\"left\", va=\"bottom\", transform=ax.transAxes)\n",
    "\n",
    "# g.map(label, \"Normalized_Q\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.figure.subplots_adjust(hspace=-.5)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# g.set_xlabels(r'Similarity [$Q_{\\mathrm{norm}}$]', fontsize=23)\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_fontsize(18)\n",
    "\n",
    "\n",
    "# g.axes[2, 0].set_ylabel(r\"Probability Density [PD$(Q_{\\mathrm{norm}}$)]\", fontsize=23)\n",
    "\n",
    "# # Modify y-tick labels font size for the last axis\n",
    "# for label in g.axes[3, 0].get_yticklabels():\n",
    "#     label.set_fontsize(18)\n",
    "\n",
    "# custom_labels = ['MiChroM+Full', 'Homopolymer']\n",
    "# plt.legend( labels=custom_labels, loc='upper left')\n",
    "# plt.ylabel(r\"Probability Density [PD$(Q_{\\mathrm{norm}}$)]\")\n",
    "# plt.xlabel(r'Similarity [$Q_{\\mathrm{norm}}$]')\n",
    "plt.xlim(0.0, 1.0)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "\n",
    "# plt.savefig('/mnt/d/Mammoth/Data_Final/PQ_Qnorm_ridge_3.eps',dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = sns.choose_colorbrewer_palette('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.choose_cubehelix_palette(as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.violinplot(data=df_filtered, x='Type', y='Normalized_Q', palette='muted')\n",
    "# plt.title('Normalized Distribution of Selected Data')\n",
    "# plt.ylabel('Normalized Value')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     'MiChroM': Rg_M_M,\n",
    "#     'Full': Rg_F_F,\n",
    "#     'HP': Rg_H_H\n",
    "# })\n",
    "\n",
    "# df_long = df.melt(var_name='Type', value_name='Rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.kdeplot(data=df_long, x='Rg', hue='Type', fill=True)\n",
    "# plt.title('Distribution of Rg for Different Types')\n",
    "# plt.xlabel('Rg')\n",
    "# plt.ylabel('Density')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = {1: 'orange', 2: 'green', 3: 'red', 4: 'blue', 5: 'purple', 6: 'brown', 7: 'pink', 8: 'gray', 9: 'olive', 10: 'cyan'}\n",
    "# c=[colors[i] for i in T ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # initialize MDS with number of dimensions as 2\n",
    "# mds = manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "\n",
    "# # fit and transform the similarity matrix\n",
    "# X_transformed = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# # plot the transformed data\n",
    "# plt.scatter(X_transformed[:, 0], X_transformed[:, 1],c=[colors[i] for i in T ])\n",
    "# # plt.savefig('/mnt/d/DT40/Centromere-Modeling/Data_Gro/Figures/Dendogram_3D_Images/MDS_sim_chr5_1000_set1.jpg',dpi=300)\n",
    "# # plt.scatter(X_transformed[:, 0], X_transformed[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kabsch(P, Q):\n",
    "    \"\"\"\n",
    "    P and Q are N x 3 matrices.\n",
    "    Returns the optimal rotation matrix R for aligning points in P to points in Q.\n",
    "    \"\"\"\n",
    "    # Center the points about the origin\n",
    "    centroid_P = np.mean(P, axis=0)\n",
    "    centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "    P_centered = P - centroid_P\n",
    "    Q_centered = Q - centroid_Q\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    H = np.dot(P_centered.T, Q_centered)\n",
    "    \n",
    "    # Singular Value Decomposition\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    \n",
    "    # Ensure a right-handed coordinate system\n",
    "    d = (np.linalg.det(U) * np.linalg.det(Vt)) < 0.0\n",
    "    \n",
    "    if d:\n",
    "        S[-1] = -S[-1]\n",
    "        U[:, -1] = -U[:, -1]\n",
    "    \n",
    "    # Calculate the optimal rotation matrix\n",
    "    R = np.dot(U, Vt)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_with_translation(P, Q):\n",
    "    \"\"\"\n",
    "    P and Q are N x 3 matrices.\n",
    "    Returns the translated and rotated version of Q that aligns with P.\n",
    "    \"\"\"\n",
    "    centroid_P = np.mean(P, axis=0)\n",
    "    centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "    P_centered = P - centroid_P\n",
    "    Q_centered = Q - centroid_Q\n",
    "    \n",
    "    R = kabsch(P_centered, Q_centered)  # Assuming you've defined the kabsch function\n",
    "    \n",
    "    # Apply rotation\n",
    "    Q_rotated = np.dot(Q_centered, R.T)\n",
    "    \n",
    "    # Translate the rotated Q to align centroids\n",
    "    Q_aligned = Q_rotated + centroid_P\n",
    "    \n",
    "    return Q_aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "P = XYZ_MiChroM[0]\n",
    "Q = XYZ_HP[10]\n",
    "# Q = XYZ_MiChroM[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = kabsch(P, Q)\n",
    "# Q_rotated = np.dot(Q, R.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_aligned = kabsch_with_translation(P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for averaged_structure\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(Q[:, 0], Q[:, 1], Q[:, 2], marker='o')\n",
    "ax1.scatter(P[:, 0], P[:, 1], P[:, 2], marker='o',color='red')\n",
    "ax1.set_xlabel(\"X\")\n",
    "ax1.set_ylabel(\"Y\")\n",
    "ax1.set_zlabel(\"Z\")\n",
    "\n",
    "# Plot for reduced_structure\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(Q_aligned[:, 0], Q_aligned[:, 1], Q_aligned[:, 2], marker='o')\n",
    "ax2.scatter(P[:, 0], P[:, 1], P[:, 2], marker='o',color='red')\n",
    "ax2.set_xlabel(\"X\")\n",
    "ax2.set_ylabel(\"Y\")\n",
    "ax2.set_zlabel(\"Z\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm_score(P_aligned, Q_aligned, L_target):\n",
    "    \"\"\"\n",
    "    P_aligned and Q_aligned are N x 3 matrices of aligned points.\n",
    "    L_target is the length of the target protein or structure.\n",
    "    \"\"\"\n",
    "    # Determine the scaling factor d0\n",
    "    d0 = 1.24 * np.sqrt(L_target - 15)\n",
    "    d0 = min(d0, 3.0)\n",
    "\n",
    "    # Calculate the distance between each pair of aligned points\n",
    "    distances = np.sqrt(np.sum((P_aligned - Q_aligned)**2, axis=1))\n",
    "\n",
    "    # Compute the TM-score\n",
    "    score = np.sum(1.0 / (1.0 + (distances / d0)**2)) / L_target\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_score(P, Q_aligned, len(P)),tm_score(P, Q, len(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_Q(P, Q, 481, 0, sigma=1.0),compute_Q(P, Q_aligned, 481, 0, sigma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords1 = np.array([[1.2, 3.4, 1.5],[4.0, 2.8, 3.7],[1.2, 4.2, 4.3],[0.0, 1.0, 2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords2 = np.array([[2.3, 7.4, 1.5],[4.0, 2.9, -1.7],[1.2, 4.2, 4.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def best_alignment_and_tm_score(P, Q):\n",
    "#     \"\"\"\n",
    "#     Finds the best alignment of a subset of Q to P using the TM-score, or vice versa.\n",
    "#     This function can handle cases where either P or Q is shorter.\n",
    "#     \"\"\"\n",
    "#     if len(P) > len(Q):\n",
    "#         P, Q = Q, P  # Swap P and Q if P is longer\n",
    "    \n",
    "#     best_score = -np.inf\n",
    "#     best_alignment = None\n",
    "\n",
    "#     for i in range(len(Q) - len(P) + 1):\n",
    "#         Q_subset = Q[i:i+len(P)]\n",
    "        \n",
    "#         Q_aligned = kabsch_with_translation(P, Q_subset)\n",
    "        \n",
    "#         score = tm_score(P, Q_aligned, len(P))\n",
    "        \n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_alignment = Q_aligned\n",
    "            \n",
    "#     return best_alignment, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_align, score = best_alignment_and_tm_score(coords2, coords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_align, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def kabsch(P, Q):\n",
    "#     # Centroid calculation\n",
    "#     centroid_P = np.mean(P, axis=0)\n",
    "#     centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "#     P_centered = P - centroid_P\n",
    "#     Q_centered = Q - centroid_Q\n",
    "\n",
    "#     # Calculate covariance matrix\n",
    "#     C = np.dot(P_centered.T, Q_centered)\n",
    "\n",
    "#     # Singular value decomposition\n",
    "#     V, _, W = np.linalg.svd(C)\n",
    "#     if np.linalg.det(V) * np.linalg.det(W) < 0:\n",
    "#         W[2, :] *= -1\n",
    "\n",
    "#     # Calculate the rotation matrix\n",
    "#     R = np.dot(V, W)\n",
    "\n",
    "#     return R\n",
    "\n",
    "# def kabsch_with_translation(P, Q):\n",
    "#     centroid_P = np.mean(P, axis=0)\n",
    "#     centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "#     P_centered = P - centroid_P\n",
    "#     Q_centered = Q - centroid_Q\n",
    "    \n",
    "#     R = kabsch(P_centered, Q_centered)\n",
    "    \n",
    "#     # Apply rotation\n",
    "#     Q_rotated = np.dot(Q_centered, R.T)\n",
    "    \n",
    "#     # Translate the rotated Q to align centroids\n",
    "#     Q_aligned = Q_rotated + centroid_P\n",
    "    \n",
    "#     return Q_aligned\n",
    "\n",
    "# def tm_score(P_aligned, Q_aligned, L_target):\n",
    "#     if L_target < 15:\n",
    "#         d0 = 1.0  # arbitrary small value, but you can adjust as needed\n",
    "#     else:\n",
    "#         d0 = 1.24 * np.sqrt(L_target - 15)\n",
    "#     d0 = min(d0, 3.0)\n",
    "#     distances = np.sqrt(np.sum((P_aligned - Q_aligned)**2, axis=1))\n",
    "#     score = np.sum(1.0 / (1.0 + (distances / d0)**2)) / L_target\n",
    "#     return score\n",
    "\n",
    "\n",
    "# def best_alignment_and_tm_score(P, Q):\n",
    "#     if len(P) > len(Q):\n",
    "#         P, Q = Q, P  # Swap P and Q if P is longer\n",
    "    \n",
    "#     best_score = -np.inf\n",
    "#     best_alignment = None\n",
    "\n",
    "#     for i in range(len(Q) - len(P) + 1):\n",
    "#         Q_subset = Q[i:i+len(P)]\n",
    "        \n",
    "#         Q_aligned = kabsch_with_translation(P, Q_subset)\n",
    "        \n",
    "#         score = tm_score(P, Q_aligned, len(P))\n",
    "        \n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_alignment = Q_aligned\n",
    "            \n",
    "#     return best_alignment, best_score\n",
    "\n",
    "# # Your provided coordinates\n",
    "# coords1 = np.array([[1.2, 3.4, 1.5], [4.0, 2.8, 3.7], [1.2, 4.2, 4.3], [0.0, 1.0, 2.0]])\n",
    "# coords2 = np.array([[2.3, 7.4, 1.5], [4.0, 2.9, -1.7], [1.2, 4.2, 4.3]])\n",
    "\n",
    "# best_align, score = best_alignment_and_tm_score(coords2, coords1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_align, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def kabsch(P, Q):\n",
    "#     centroid_P = np.mean(P, axis=0)\n",
    "#     centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "#     P_centered = P - centroid_P\n",
    "#     Q_centered = Q - centroid_Q\n",
    "\n",
    "#     C = np.dot(P_centered.T, Q_centered)\n",
    "\n",
    "#     V, _, W = np.linalg.svd(C)\n",
    "#     if np.linalg.det(V) * np.linalg.det(W) < 0:\n",
    "#         W[2, :] *= -1\n",
    "\n",
    "#     R = np.dot(V, W)\n",
    "#     return R\n",
    "\n",
    "# def kabsch_with_translation(P, Q):\n",
    "#     centroid_P = np.mean(P, axis=0)\n",
    "#     centroid_Q = np.mean(Q, axis=0)\n",
    "    \n",
    "#     P_centered = P - centroid_P\n",
    "#     Q_centered = Q - centroid_Q\n",
    "    \n",
    "#     R = kabsch(P_centered, Q_centered)\n",
    "#     Q_rotated = np.dot(Q_centered, R.T)\n",
    "#     Q_aligned = Q_rotated + centroid_P\n",
    "    \n",
    "#     return Q_aligned\n",
    "\n",
    "# def tm_score(P_aligned, Q_aligned, L_target):\n",
    "#     if L_target < 15:\n",
    "#         d0 = 1.0\n",
    "#     else:\n",
    "#         d0 = 1.24 * np.sqrt(L_target - 15)\n",
    "#     d0 = min(d0, 3.0)\n",
    "    \n",
    "#     distances = np.sqrt(np.sum((P_aligned - Q_aligned)**2, axis=1))\n",
    "#     score = np.sum(1.0 / (1.0 + (distances / d0)**2)) / L_target\n",
    "#     return score\n",
    "\n",
    "# def best_alignment_and_tm_score(P, Q):\n",
    "#     if len(P) > len(Q):\n",
    "#         P, Q = Q, P\n",
    "    \n",
    "#     best_score = -np.inf\n",
    "#     best_alignment = None\n",
    "\n",
    "#     for i in range(len(Q) - len(P) + 1):\n",
    "#         Q_subset = Q[i:i+len(P)]\n",
    "#         Q_aligned = kabsch_with_translation(P, Q_subset)\n",
    "        \n",
    "#         score = tm_score(P, Q_aligned, len(P))\n",
    "        \n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_alignment = Q_aligned\n",
    "            \n",
    "#     return best_alignment, best_score\n",
    "\n",
    "# coords1 = np.array([[1.2, 3.4, 1.5], [4.0, 2.8, 3.7], [1.2, 4.2, 4.3], [0.0, 1.0, 2.0]])\n",
    "# coords2 = np.array([[2.3, 7.4, 1.5], [4.0, 2.9, -1.7], [1.2, 4.2, 4.3]])\n",
    "\n",
    "# best_align, score = best_alignment_and_tm_score(coords1, coords2)\n",
    "\n",
    "# best_align_output = best_align.tolist()\n",
    "# score_output = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = []\n",
    "# for i in range(1, 11):\n",
    "#     sequence.append(i)\n",
    "#     sequence.append('B')\n",
    "# for i in range(11, 21):\n",
    "#     sequence.append(i)\n",
    "#     sequence.append('A')\n",
    "# for i in range(21, 31):\n",
    "#     sequence.append(i)\n",
    "#     sequence.append('C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = sequence[::2]\n",
    "# patches = sequence[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polymer_structure = np.random.rand(30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(indices),len(patches),np.shape(polymer_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_patch_type = patches[0]\n",
    "# patch_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged_structure = np.zeros_like(polymer_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, patch in enumerate(patches):\n",
    "#     if patch == current_patch_type:\n",
    "#         patch_indices.append(i)\n",
    "#     else:\n",
    "#         # When the patch changes, calculate the COM of the current patch\n",
    "#         COM = np.mean(polymer_structure[patch_indices, :], axis=0)\n",
    "        \n",
    "#         for index in patch_indices:\n",
    "#             averaged_structure[index, :] = COM\n",
    "            \n",
    "#         patch_indices = [i]\n",
    "#         current_patch_type = patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle the last patch\n",
    "# if patch_indices:\n",
    "#     COM = np.mean(polymer_structure[patch_indices, :], axis=0)\n",
    "#     for index in patch_indices:\n",
    "#         averaged_structure[index, :] = COM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_patch_type = patches[0]\n",
    "# patch_indices = []\n",
    "\n",
    "# # This list will store the new reduced polymer structure\n",
    "# reduced_structure = []\n",
    "# reduced_sequence = []\n",
    "# for i, patch in enumerate(patches):\n",
    "#     if patch == current_patch_type:\n",
    "#         patch_indices.append(i)\n",
    "#     if patch != current_patch_type or i == len(patches) - 1:  # Adding condition for the last element\n",
    "#         # When the patch changes or we are at the end, calculate the COM of the current patch\n",
    "#         COM = np.mean(polymer_structure[patch_indices, :], axis=0)\n",
    "#         reduced_structure.append(COM)\n",
    "#         reduced_sequence.append(current_patch_type)\n",
    "        \n",
    "#         patch_indices = [i]\n",
    "#         current_patch_type = patch\n",
    "\n",
    "# reduced_structure = np.array(reduced_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot for averaged_structure\n",
    "# ax1 = fig.add_subplot(121, projection='3d')\n",
    "# ax1.scatter(averaged_structure[:, 0], averaged_structure[:, 1], averaged_structure[:, 2], marker='o')\n",
    "# ax1.set_title(\"Averaged Structure\")\n",
    "# ax1.set_xlabel(\"X\")\n",
    "# ax1.set_ylabel(\"Y\")\n",
    "# ax1.set_zlabel(\"Z\")\n",
    "\n",
    "# # Plot for reduced_structure\n",
    "# ax2 = fig.add_subplot(122, projection='3d')\n",
    "# ax2.scatter(reduced_structure[:, 0], reduced_structure[:, 1], reduced_structure[:, 2], marker='o')\n",
    "# ax2.set_title(\"Reduced Structure\")\n",
    "# ax2.set_xlabel(\"X\")\n",
    "# ax2.set_ylabel(\"Y\")\n",
    "# ax2.set_zlabel(\"Z\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
