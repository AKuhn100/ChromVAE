#!/usr/bin/env python3
"""
Script to sample from trained VAE latent space and generate PDB files.

This script loads a trained VAE model and generates new chromosome structures
by sampling from the latent space and decoding them back to coordinate space.
"""

import torch
import numpy as np
from pathlib import Path
import argparse
from typing import List, Tuple

# Add project root to path
import sys
sys.path.append(str(Path(__file__).parent.parent))

from VAE.Simple_VAE import LGL_VAE
from DataLoader.Chromosome21PDBDataset import Chromosome21PDBDataset
from Utils.Utils import Utils


def load_trained_model(model_path: str, input_dim: int, hidden_dim: int = 512, latent_dim: int = 32) -> LGL_VAE:
    """Load a trained VAE model from checkpoint."""
    utils = Utils()
    
    # Create model with same architecture as training
    model = LGL_VAE(hidden_dim=hidden_dim, latent_dim=latent_dim, input_dim=input_dim)
    model = model.to(utils.device)
    
    # Load trained weights
    checkpoint = torch.load(model_path, map_location=utils.device)
    model.load_state_dict(checkpoint)
    model.eval()
    
    print(f"Loaded trained model from {model_path}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, utils


def sample_latent_space(model: LGL_VAE, num_samples: int, device: str) -> torch.Tensor:
    """Sample random points from the latent space."""
    with torch.no_grad():
        # Get latent dimension from model
        latent_dim = model.to_mu.out_features
        
        # Sample from standard normal distribution in latent space
        z = torch.randn(num_samples, latent_dim, device=device)
        
        # Decode to coordinate space
        reconstructed = model.decoder(z)
        
    return reconstructed


def sample_interpolated_latent_space(model: LGL_VAE, num_samples: int, device: str) -> torch.Tensor:
    """Sample interpolated points between random latent vectors."""
    with torch.no_grad():
        # Get latent dimension from model
        latent_dim = model.to_mu.out_features
        
        # Generate two random latent vectors
        z1 = torch.randn(1, latent_dim, device=device)
        z2 = torch.randn(1, latent_dim, device=device)
        
        # Create interpolation points
        alphas = torch.linspace(0, 1, num_samples, device=device).view(-1, 1)
        z_interpolated = (1 - alphas) * z1 + alphas * z2
        
        # Decode to coordinate space
        reconstructed = model.decoder(z_interpolated)
        
    return reconstructed


def coordinates_to_pdb(coordinates: np.ndarray, output_path: str, model_id: int = 1) -> None:
    """Convert coordinate array to PDB format."""
    num_atoms = len(coordinates) // 3
    
    with open(output_path, 'a') as f:  # Append mode
        f.write(f"MODEL        {model_id:4d}\n")
        
        for i in range(num_atoms):
            x, y, z = coordinates[i*3:(i+1)*3]
            # Write ATOM record in PDB format
            f.write(f"ATOM  {i+1:5d}  CA  UNK A{i+1:4d}    {x:8.3f}{y:8.3f}{z:8.3f}  1.00  0.00           C\n")
        
        f.write("ENDMDL\n")


def generate_samples(
    model_path: str,
    dataset_path: str,
    output_dir: str,
    num_samples: int = 10,
    sample_type: str = "random",
    hidden_dim: int = 512,
    latent_dim: int = 32
) -> None:
    """Generate samples from trained VAE and save as PDB files."""
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Load dataset to get input dimensions
    print(f"Loading dataset from {dataset_path}")
    dataset = Chromosome21PDBDataset(
        pdb_path=dataset_path,
        record_types=("ATOM",),
        center=True,
        scale=1.0
    )
    
    input_dim = dataset.vector_length
    print(f"Input dimension: {input_dim}")
    
    # Load trained model
    model, utils = load_trained_model(model_path, input_dim, hidden_dim, latent_dim)
    
    # Generate samples
    print(f"Generating {num_samples} samples using {sample_type} sampling...")
    
    if sample_type == "random":
        samples = sample_latent_space(model, num_samples, utils.device)
    elif sample_type == "interpolated":
        samples = sample_interpolated_latent_space(model, num_samples, utils.device)
    else:
        raise ValueError(f"Unknown sample_type: {sample_type}")
    
    # Convert to numpy and save as single PDB file with multiple models
    samples_np = samples.cpu().numpy()
    
    # Create single output PDB file
    output_file = output_path / f"generated_samples_{sample_type}.pdb"
    
    # Clear the file first
    with open(output_file, 'w') as f:
        f.write("HEADER    GENERATED CHROMOSOME STRUCTURES FROM VAE LATENT SPACE\n")
        f.write("REMARK    Each MODEL represents one sampled point from the latent space\n")
        f.write("REMARK    Generated by ChromatinVAE sampling script\n")
    
    # Add each sample as a model
    for i, sample in enumerate(samples_np):
        coordinates_to_pdb(sample, str(output_file), model_id=i+1)
        print(f"Added sample {i+1} as MODEL {i+1}")
    
    print(f"\nGenerated {num_samples} samples in single PDB file: {output_file}")
    print("You can visualize this PDB file using PyMOL, VMD, or other molecular viewers.")
    print("Each MODEL in the file represents one sampled structure from the latent space.")


def main():
    parser = argparse.ArgumentParser(description="Generate PDB samples from trained VAE")
    parser.add_argument("--model_path", type=str, default="./outputs/ChromatinVAE/trained_vae_model.pt",
                       help="Path to trained model checkpoint")
    parser.add_argument("--dataset_path", type=str, default="./Data/chromosome21_aligned.pdb",
                       help="Path to original PDB dataset")
    parser.add_argument("--output_dir", type=str, default="./outputs/Generated_Samples",
                       help="Directory to save generated PDB files")
    parser.add_argument("--num_samples", type=int, default=10,
                       help="Number of samples to generate")
    parser.add_argument("--sample_type", type=str, default="random", choices=["random", "interpolated"],
                       help="Type of sampling: random or interpolated")
    parser.add_argument("--hidden_dim", type=int, default=4096,
                       help="Hidden dimension of the model")
    parser.add_argument("--latent_dim", type=int, default=2,
                       help="Latent dimension of the model")
    
    args = parser.parse_args()
    
    # Check if model exists
    if not Path(args.model_path).exists():
        print(f"Error: Model file not found at {args.model_path}")
        return
    
    # Check if dataset exists
    if not Path(args.dataset_path).exists():
        print(f"Error: Dataset file not found at {args.dataset_path}")
        return
    
    generate_samples(
        model_path=args.model_path,
        dataset_path=args.dataset_path,
        output_dir=args.output_dir,
        num_samples=args.num_samples,
        sample_type=args.sample_type,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim
    )


if __name__ == "__main__":
    main()
